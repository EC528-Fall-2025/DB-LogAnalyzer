Event Name
Description
Internal Role in FoundationDB

MoveInUpdatesPrePersist
Indicates a Redwood storage engine operation where in-memory updates are being merged (“moved in”) just before being persisted to disk. This marks a step in Redwood’s commit path where mutable updates are flushed into the B-tree structure.

Part of the storage engine (Redwood) lifecycle – it occurs during transaction commit on a storage server, just before writing to disk, helping ensure recent updates are included in the persistent structure. (Redwood’s internal flush phase)

PingLatency
A periodic trace event measuring network ping latencies between FDB peers. It logs stats like min/mean ping times, bytes sent/received, and connection attempts . For example, it shows if one process can’t reach another and how long it has been trying.
Part of the network coordination layer (FlowTransport): it monitors health of peer connections and can reveal connectivity issues or slow links . A high PingLatency or many failed attempts indicates communication problems between servers.

Sim2Connection
Logged in the simulation environment’s network (Sim2). It represents an outgoing connection event in the simulated network stack. In simulation, connections are virtual; this event occurs when a simulated process “connects” to another, and it checks partition conditions (e.g. g_clogging) .
Part of deterministic simulation: Sim2Connection events are used in testing to model network behavior. They help developers verify how code handles new connections under chaotic conditions (like forced partitions or delays) without affecting real networks .

ConnectionKeeper
This event comes from the connectionKeeper actor in Flow’s networking. It logs attempts to keep a connection alive or to reconnect to a peer . Essentially it’s a heartbeat/retry mechanism’s trace event.
Part of the network layer and possibly config database broadcasting: ConnectionKeeper ensures continuous contact with critical peers (e.g. coordinators). Its events indicate the system is retrying a lost connection or maintaining one, crucial for cluster coordination and failover handling.

VFSAsyncFileOpened
Marks that a Virtual File System (VFS) asynchronous file has been opened. It logs the initiation of an async file (filename, flags, etc.) when using Flow’s I/O abstraction.
Part of the storage engine I/O: The Flow I/O layer uses async files for non-blocking disk operations. This event confirms a data file (for logs or storage) was successfully opened asynchronously, aiding debugging of file availability.

VFSAsyncFileConstruct
Logged when a VFSAsyncFile object is constructed. It indicates the in-memory creation of a wrapper for an OS file handle.
Storage engine / I/O layer: Shows that an async file interface is set up (though not yet opened on disk). Useful for tracking resource allocation of files (e.g. a new log or data file structure being prepared).

DumpToken
A debug event that dumps a unique “token” identifier for certain asynchronous operations or actor endpoints. For example, when a role is recruited, FDB logs a DumpToken with a name (like recruited.waitFailure or recruited.commit) and a token value . This helps correlate request–response pairs.
Part of cluster coordination and failure monitoring: It is used internally to track actor lifecycle. E.g., the cluster controller uses tokens to wait on failures of recruited processes – DumpToken logs those tokens (IDs) so if a process dies or an action completes, the corresponding waiting actor is identifiable . It’s purely for debugging/tracing logic flow.
ChaosMetrics
Gathers metrics during simulated “Chaos” fault injection on the disk I/O subsystem. For instance, when the special AsyncFileChaos layer introduces delays or errors, it increments counters (like diskDelays) in ChaosMetrics . This event reports those counters.
Used in simulation and testing: it measures how many artificial faults (like delayed or failed disk ops) were injected. Internally, it ensures the chaos test is exercising the system (e.g., incrementing diskDelays count) . It has no role in production except to validate FDB’s resilience to disk slowness by logging these induced conditions.
MachineLoadDetail
A detailed breakdown of CPU usage on a machine, logged periodically. It includes user, system, idle CPU ticks, I/O wait, IRQ, etc . Essentially it’s a snapshot of /proc/stat CPU counters on the FDB process’s host.
Part of Process/Machine metrics: Helps FDB’s monitoring identify if a machine is overloaded. For example, high IOWait in MachineLoadDetail would indicate the disk is a bottleneck . It complements the coarser MachineMetrics by giving low-level CPU state info per process (especially for alerting on high usage or diagnosing slow processing).
GotServerDBInfoChange
Indicates a process received a new ServerDBInfo (the cluster’s configuration blob). It logs that the cluster’s coordination state changed – e.g. new master or proxies – by including a ChangeID and new role IDs (MasterID, RatekeeperID, etc.) .
Part of cluster coordination: This event fires on all FDB processes when the cluster controller broadcasts an updated configuration (often after recovery). Internally, it triggers each process to update its notion of cluster topology. In other words, FDB nodes log GotServerDBInfoChange when they learn of new coordinators, proxies, logs, etc., which is essential to the transaction lifecycle (clients get new proxy list, storage gets new TLog list, etc.) .
VFSAsyncFileDestroy
Logged when a VFS async file object is destroyed (closed). It means an async file that was open is now being cleaned up and its resources freed.
Part of storage I/O cleanup: ensures that file handles are closed and any pending I/O is finished. This event lets operators know the database closed a data or log file (for example, after switching files or shutting down). Useful in debugging file descriptor leaks or rotation of log files.
VFSAsyncFileDestroyStart
Similar to above but marks the start of destruction. It might be logged just before actually closing the file, possibly to measure how long destruction takes if paired with a corresponding “destroy” event.
I/O layer: indicates the system has initiated closing of an async file (perhaps flushing buffers). It’s an instrumented point to detect stalls during file closure (e.g., if closing a large file or waiting for OS).
StorageMetrics
A periodic metric report from each Storage Server with stats about its workload. Fields include number of queries (get key/value/range), bytes read/written, mutations applied, queue lengths, etc . For example, it tracks how many read operations were finished and how many bytes were fetched.
Part of the storage engine and data distribution system: The Data Distributor uses StorageMetrics to gauge each storage server’s load (e.g., bytes stored, read hotness) for rebalancing. Internally, the storage server updates these counters continuously and logs them at intervals . This helps maintain the transaction read path (by informing where hot spots are) and to decide shard movements.
BusiestWriteTag
Logs which transaction tag had the highest write cost in the cluster recently. FDB’s ratekeeper monitors transaction tags (for throttling hot workloads) and periodically outputs the tag consuming the most write bandwidth. The event includes the tag name and its “cost.”
Part of Ratekeeper (load regulation): It identifies write-heavy transactions for potential throttling. For instance, if a particular tag is generating disproportionate writes, BusiestWriteTag (and *ReadTag) events fire to note that . Internally, this ties into the transaction subsystem by enabling automatic throttling of the busiest tag to protect storage servers.
BusiestReadTag
Similar to above, but for read traffic. It reports the tag responsible for the largest read load. This can indicate a read hotspot by tag.
Ratekeeper / Performance monitoring: Helps detect if one tenant or transaction type is saturating read capacity. FDB can use this info to suggest or apply throttling. It’s part of ensuring fairness in the transaction lifecycle – heavy read-tag traffic can be isolated and managed .
Sim2IncomingConn
A simulation event representing an incoming connection in the Sim2 network. When a simulated FDB process “accepts” a connection from another, this event is logged (subject to any simulated partitions).
Deterministic simulation: Works with Sim2Connection; it models the server side of a new connection. It ensures that in simulation, both ends of a connection are traced. This is crucial for testing scenarios like network partitions, where an incoming side might see a disconnect if g_clogginghas partitioned it (Sim2IncomingConn will not complete in that case) .
SQLiteDBCreate
Logged when the SQLite storage engine (the older storage engine in FDB) is initialized or a new database file is created. It means a storage server opened or created its .sqlite file for the key-value store.
Part of storage engine startup: In versions or configurations that use SQLite, this marks the beginning of persistence. Internally, it indicates that the process has set up the local database file for reads/writes. (In newer versions using Redwood or RocksDB, this might not appear, or only in special cases like test storage engines.)
KVThreadInitTime
Measures the initialization time of the storage engine’s key-value storage thread. Many storage engines (Redwood, RocksDB) run background threads; this event logs how long it took to start those threads.
Part of storage server startup: A storage server may spawn a thread to handle disk I/O or commit queue. KVThreadInitTime provides insight into any delay in launching that thread. If this is high, the storage engine took long to become ready, which could affect recovery time.
Knob
An event indicating that a runtime knob (configuration parameter) was set or changed. It typically includes the knob name and new value. For example, if an operator changes a knob via the special transaction (global config), each process logs a “Knob” event reflecting the update.
Part of dynamic configuration: It ties into the client/server operations by allowing FDB to tune behaviors without restart. Internally, when a knob like WORKER_LOGGING_INTERVAL is adjusted, the processes log it to confirm the change took effect . This ensures visibility of configuration changes that alter transaction or storage behavior on the fly.
IncomingConnection
Logged by a server when a new network connection is accepted from another FDB process or client. It often details the source address. In trace logs it may appear as “ConnectionFrom” or “IncomingConnection” with an address detail.
Part of the FDB networking: it marks the start of a new peer communication channel. Internally, this is crucial in the client/server handshake – e.g., when a storage server accepts a connection from a log or client, it notes it. Monitoring these events can help identify connection churn or excessive reconnects in the cluster .
CoroStop
Marks that a Flow coroutine (actor) has yielded or stopped execution. FDB’s Flow might use a coroutine implementation for certain tasks (in newer versions, some experiments use true coroutine switching). This event likely logs an actor context being paused.
Part of the Flow runtime scheduling: It’s an instrument for developer insight – when an actor stops (e.g., waiting on I/O), CoroStop is emitted. Internally it doesn’t affect logic but helps measure context-switching behavior or stalls in the event loop. (It’s mostly relevant if new coroutine-based Flow enhancements are enabled.)
SQLiteDBDestroy
Logged when a SQLite-based storage engine is closed or destroyed. It indicates the database file is being closed (usually during storage server shutdown or role change).
Storage engine shutdown: Ensures that the SQLite environment is cleanly torn down. Internally, it marks that the storage server is no longer using the SQLite file – no more commits or reads will occur. This event is important during role transitions (SS role removed) to avoid file corruption.
Role
A generic event logging role changes or refreshes on a process. It contains details like the Transition (“Add”, “Remove”, or periodic “Refresh”) and the role name (“StorageServer”, “Worker”, etc.) . FDB logs this every few seconds for active roles (Refresh) and whenever roles are added/removed.
Part of cluster coordination and process roles: The cluster controller assigns roles (CC, Logs, SS, Proxy, etc.) to processes. The “Role” events track these assignments. For example, on recruiting a new storage server, a Role event with Transition=Add and As=StorageServer is logged. The periodic “Refresh” (every ~5s) helps monitoring systems keep an up-to-date view of what roles each process has . This improves observability but can be verbose (as noted by operators) .
DDRecruitExcl1
A Data Distributor event likely indicating “Data Distributor recruit attempt with exclusion – pass 1”. When forming new storage teams, the DD may try to recruit storage servers while excluding certain ones (e.g., those in the same zone). This event denotes such an attempt.
Part of data distribution (team building): It suggests the algorithm is attempting to recruit a storage server for a team but had to exclude at least one candidate (perhaps because of locality or prior failures). “Excl1” implies this is the first exclusion criteria used. Internally, the Data Distributor might log multiple steps (Excl1, Excl2…) if it can’t find a candidate meeting all requirements (like unique zone, machine, etc.), thereby documenting its process of finding a new storage server.
TLogPopDetails
Provides details when a TLog “pop” occurs. Transaction logs keep track of the oldest data that storage servers still need. When storage servers have persisted a version, they send a pop request allowing logs to drop (discard) old mutations. This event logs which versions/tags were popped and how much data was freed.
Part of transaction log (TLog) lifecycle: It’s crucial for replication cleanup. Internally, TLogPopDetails allows the system to trace the progress of log truncation – ensuring that TLogs only keep necessary history. It might include the end version popped to and counts of mutations freed, helping debug if logs aren’t freeing space as expected.
CodeCoverage
A special event related to test runs, indicating code coverage collection points. FDB’s simulation test framework can be built to log code coverage; this event marks where coverage info is dumped or a particular section was executed in simulation.
Part of testing instrumentation: Not used in production, but when running correctness tests with coverage enabled, CodeCoverage events help developers see which code paths were taken . It has no effect on the transaction or storage logic; it purely aids in verifying test completeness by writing coverage data.
RelocateShard
Indicates that the Data Distributor has queued or is processing a shard relocation . It often carries the priority of the move and a RelocationID. Essentially, FDB is moving a key-range (shard) to better balance load or heal the cluster.
Part of data distribution: This is central to the replication and rebalancingsubsystem. A RelocateShard event means a range of keys is being transferred between storage server teams to maintain balance or fault tolerance . The priority field (e.g., 120 for rebalance, or 700+ for failure-driven) indicates why – high priority might mean a server died (urgent data movement), whereas lower means routine rebalance .
RelocateShard_StartMoveKeys
Signifies the start of the “move keys” phase for a shard relocation. Once a RelocateShard is decided, FDB actually begins moving the keys in that range; this event is logged at that moment. It may include the shard’s range and destination team.
Data distribution: It marks the transition from decision to action – the DD has actually initiated copying data for the shard to its new location. Internally, this corresponds to the MoveKeys actor being launched for that shard, which will orchestrate fetching all the shard’s data from source to destination storage servers. Logging this helps track progress of individual shard moves and identify slow ones.
SlowSSLoopx100
A warning that the Storage Server’s main loop has been slow repeatedly. It’s logged after 100 consecutive slow iterations of the storage server run loop . The event includes an “Elapsed” time representing how long an iteration took, which exceeded a threshold (the “x100” means it suppressed 99 prior warnings).
Part of storage server performance: It indicates the storage server is struggling (perhaps due to disk stall). Internally, a storage server processes incoming mutations and reads in a loop – if this loop is consistently taking too long (e.g., >50ms each) it triggers this event . This often correlates with slow disks or an overwhelmed CPU. It’s crucial for the transaction subsystem because a slow storage server can increase read latencies and delay transaction commits (since it won’t fetch from TLogs fast enough).
TLogPoppedTag
Logged by a TLog to note that a specific tag’s data has been popped (removed) up to a certain version. Tags correspond to replication streams (e.g., for each storage team or backup). This event isolates the pop information per tag.
Part of TLog replication housekeeping: It provides fine-grained visibility into log truncation. For example, if a backup tag (for DR) is far behind, other tags might pop sooner. TLogPoppedTag events let operators see which replication stream (storage server, log router, etc.) just advanced. Internally it confirms that, say, storage server X has now persisted all data through version V, so the log dropped that data for that tag.
GetLogTeamWorkerUnavailable
A trace event (likely on the cluster controller) indicating that it attempted to form or query a log team (the group of log processes for replication) but a worker wasn’t available. This could occur during recruitment of TLogs when there aren’t enough eligible processes free.
Part of recovery/coordination: When configuring TLogs (transaction logs) for the database, the cluster controller must recruit processes. If none are available (for example, all processes are occupied or exclusion rules prevent selection), it logs this. Internally, it informs that the desired redundancy for logs can’t immediately be achieved. This event would precede retries or ultimately a smaller log team until more processes join. It’s a sign of resource exhaustion in the transaction logging replication component.
TLogPeekSingleNoHistory
This event describes a particular strategy used when storage servers or log routers read (peek) from the TLog. “SingleNoHistory” suggests the peek request was served by a single log and did not require historical (older spilled) data. Possibly, it means the storage server read from one active log and got all data without needing any older segments.
Part of transaction log -> storage replication: FDB tries to serve replication data efficiently. This trace indicates the outcome of a peek optimization – by avoiding reading duplicate or history data, it minimized disk I/O. Internally, the TLog and storage server negotiation decided that only one log replica needed to be read (maybe because that log had all relevant mutations in memory). It relates to performance optimization of pulling mutations for storage servers.
TLogPeekLocalBestOnly
A peek optimization event likely relevant in multi-region configurations. “LocalBestOnly” implies the data distribution decided to fetch from local TLogs only (the “best” local log) rather than also peeking remote logs or all logs. For example, in a multi-DC setup, a storage server might first try the local transaction logs for mutations.
Part of multi-region replication: This event means FDB attempted to get replication data from the closest source to reduce latency. Internally, if the local region’s logs have the data (and are not far behind), the storage server will use them (“best effort local”). This improves the transaction commit speed by avoiding cross-region reads. The event confirms that approach was taken for that peek cycle.
ClogInterface
Logged by the simulator when it introduces a network partition on an interface. “ClogInterface” is literally the simulator “clogging” (i.e. blocking) communication for a given IP or process for a duration . When this event appears, it often includes which address is being partitioned.
Part of fault injection (simulation): Internally, g_simulator->clogInterface(...)creates network faults . The event notifies that a particular simulated machine or process will not be able to send/receive messages (as if its network cable were unplugged). This is crucial in testing FDB’s coordination and failover logic under network partitions. In real clusters, this does not occur – it’s purely a test artifact.
CloggingPair
Another simulation event, likely indicating a specific pair of machines or processes that are partitioned from each other (“clogged”). It may log two addresses, showing that communications between them are now cut.
Simulation network partitioning: Whereas ClogInterface might partition one node from all, CloggingPair could partition a link between two nodes (bidirectionally). Internally, the simulator uses this to create more targeted partitions. It affects the coordination/consensus by testing scenarios like one storage can’t talk to one tLog, etc. This event helps track these specific fault injections during tests, ensuring the intended pair was indeed isolated.
AFCUnderlyingOpenBegin
Logged by the AsyncFileCached (AFC) layer when it starts opening the underlying physical file. AFC is a caching wrapper around a real file; this event marks the beginning of an open call on that real file.
Part of the storage engine’s I/O layer: It helps measure latency of file opens. Internally, when FDB needs to open (or reopen) a database file or log file, the cached layer will note when it began the OS open operation. This is useful to detect stalls (if there is a long gap between OpenBegin and OpenEnd). It’s especially relevant if many files are opened (like in RocksDB’s sharded storage).
TLogQueueCommitSlow
A warning event indicating that a TLog’s queue commit is slow. In practice, this means the TLog is taking unusually long to durably commit mutations to its queue on disk. It’s often triggered when the disk is slow or the queue is very large. It appears with SevWarnAlways (Severity 20) in logs .
Part of transaction logging: This directly impacts the transaction commit latency. When a TLog’s commit is slow, it can become a bottleneck, causing the cluster to declare that log “degraded.” Internally, the TLogQueueCommitSlow event is used by the Ratekeeper/cluster to detect slow logs and possibly reduce transaction rate. It often correlates with disk issues – e.g., an Azure disk that can’t keep up will cause this warning . Operators seeing this should investigate disk I/O or log saturation.
CleanUpRocksDBLogs
Indicates that the storage engine (when using RocksDB) is cleaning up its WAL (Write-Ahead Log) or other log files. RocksDB creates log files for recent writes; this event logs their deletion/truncation once they are no longer needed (for example, after flushing to SST files).
Part of RocksDB storage maintenance: Internally, FDB’s integration of RocksDB (specifically ShardedRocksDB) will periodically remove old WAL files to reclaim disk space. This event signals that such a cleanup ran. It relates to the storage engine lifecycle because leaving WALs around could fill disk – so this trace confirms the cleanup routine is happening (especially after heavy write bursts or large range clears that generate many log files).
SSPrivateMutation
“SS” stands for Storage Server. This event logs that a storage server applied a private mutation, which is an internal, system-defined update not directly visible to clients. Examples are metadata keys for shard management or recovering versions. It might include details of the mutation (like a key prefixed with 0xFF which denotes system keys).
Part of storage server replication & recovery: Private mutations are used during data movements (like adding a new server to a shard) or in multi-version replication (version vector maintenance). They ensure that storage servers update their local metadata (such as which shards they own). The transaction lifecycle relies on these hidden mutations to coordinate data distribution without exposing them to user transactions. SSPrivateMutation events let developers verify that these special updates are processed (for instance, applying a range clear during shard removal).
BugifySection
Logged in simulation when a BUGGIFY section is activated. FDB’s testing uses the BUGGIFY macro to randomly introduce failures in code; when a particular section triggers (i.e., a fault is injected), a BugifySection event is emitted identifying that code path.
Part of simulation fault injection: It has no role in production except ensuring code was robustly tested. Internally, it marks that the simulator took a random branch – for example, “on a 20% chance, don’t send this message” – and indeed skipped the message. BugifySection events help in debugging simulation failures by pinpointing which injected fault led to an issue.
StorageServerInitProgress
Indicates how far along a storage server is in initializing. For instance, when a storage server starts, it may have to recover its data (apply missed mutations) – this event periodically logs progress (maybe as a percentage or step count) through that process.
Part of storage server startup/recovery: It ensures that if a storage server is taking a long time to come online (perhaps due to large data or slow disk), there’s visibility into where it’s spending time. Internally, the storage server might set states like “reading old data” or “finished applying mutation logs”; StorageServerInitProgress would reflect these milestones. This helps the cluster controller decide if a storage server is healthy or stuck during recruitment.
BuildServerTeams
Logged when the Data Distributor is executing its algorithm to form new storage server teams. A “team” is a set of storage servers that store replicas of the same shard. This event likely includes how many teams were built or needs to be built.
Part of data distribution (team management): Internally, when new servers are added or some fail, FDB must (re)build teams to maintain redundancy. BuildServerTeams traces the operation where it picks combinations of servers to serve as replica sets. It relates to the replication system: ensuring each shard has teams on different zones. If this event appears frequently, the cluster could be struggling to satisfy team constraints (e.g., not enough distinct zones).
TLogMetrics
A periodic event from each TLog process reporting its internal metrics. It contains data like how many bytes are input (uncommitted) vs durable, versions, queue lengths, etc . Notably, it shows the difference between BytesInput and BytesDurable, which is the TLog’s memory queue size .
Part of transaction log subsystem: These metrics drive Ratekeeper’s decisions on throttling. Internally, if a TLog’s memory queue (BytesInput – BytesDurable) grows too large, it will trigger batch throttling at ~1.1 GB and emergency throttling at 2.0 GB . TLogMetrics also help identify if a log is lagging in disk flush. It’s essential for the transaction subsystem to ensure logs don’t overwhelm memory and that log data is being persisted promptly.
CoroUnblocked
Marks that a Flow coroutine/actor has been unblocked (resumed). It pairs with events like CoroStop – when an async task that was waiting on something (I/O, timer, etc.) becomes ready to run again, CoroUnblocked is logged to note that it’s continuing after the wait.
Part of Flow’s cooperative multitasking: It provides visibility into the scheduler. Internally, when an actor yields (wait()), Flow schedules other work until the future is ready, then logs CoroUnblocked when that actor is resumed. In potential new Flow implementations (with true coroutines), this helps ensure that scheduling is working correctly and that no actor is starved. It doesn’t affect end-user functionality but helps fine-tune the responsiveness of the actor model.
AFCUnderlyingOpenEnd
The counterpart to AFCUnderlyingOpenBegin – it logs that the underlying file open operation has completed. It often would include any error code or the file descriptor obtained. The time between OpenBegin and OpenEnd reflects file-open latency.
Storage engine I/O: Finishing the open means the database can now start using the file. Internally, the AsyncFileCached layer uses this to possibly warm up caches or log the success. If OpenEnd logs significantly later than OpenBegin, that could indicate a slow filesystem or blocking call (which is unusual since opens are usually fast, but on network filesystems it might matter).
AFCUnderlyingSize
An event reporting the size of the underlying file accessed via AsyncFileCached. For example, right after opening or extending a file, FDB may log the file’s current size on disk.
Part of storage space management: Knowing the file size is important when preallocating or when truncating files. Internally, the system might log this to decide if it should truncate (shrink) the file or not. It can be related to how Redwood or the log queue manage their files – e.g., after moving data out, they might check if file can be truncated. This event gives a trace of those decisions by showing file sizes at certain points.
AsyncFileCachedDel
Logged when the AsyncFileCached layer deletes or evicts data from its cache. The “Del” suggests that a cached block was removed (perhaps to free memory or because the file is closed). It may also be triggered when invalidating cache after writes.
Storage engine caching: Internally, FDB’s AsyncFileCached buffers recently read/written pages. This event means a page or segment was dropped. It is useful for debugging the caching behavior – e.g., if we see too many AsyncFileCachedDel events, it might indicate thrashing (cache not large enough). It ties into performance of the storage engine, as cache hits vs misses affect read latency.
CacheMetrics
Periodic metrics from the storage engine’s cache (likely Redwood’s page cache or the Sharded RocksDB block cache). It would include things like cache hit rate, misses, evictions, and usage bytes.
Part of storage engine performance: For Redwood, which has an in-memory pager cache, CacheMetrics track how effectively it’s serving reads from memory. Internally, a high miss count would correlate with more disk I/O and slower reads. In RocksDB mode, it could reflect block cache stats. These metrics inform tuning of cache size and help identify if working set fits in memory or not.
SpringCleaningMetrics
Logs metrics about background cleanup tasks on the storage server, especially for the SSD engine’sdeferred deletion of pages. It includes counters for “LazyDelete” and “Vacuuming” . LazyDelete is how many pages from cleared ranges have been processed, and Vacuuming is how many pages moved to free tail space. When these drop to ~0, it means the storage engine finished cleaning up after large deletes .
Part of storage engine maintenance: This is crucial after heavy writes or large range clears. Internally, the storage engine (SSD b-tree or Redwood) doesn’t immediately reclaim space on a clear; it marks pages free and cleans later (“spring cleaning”). SpringCleaningMetrics allows FDB to expose this background activity . If these metrics are high, the cluster may be doing a lot of cleanup I/O, which can impact transaction throughput. It gives operators insight to distinguish normal workload IOPS from post-clearance cleanup IOPS .
DiskMetrics
A periodic report of low-level disk I/O statistics from a process. It includes things like number of disk read/write operations, disk queue length, and possibly global storage engine memory usage . In the example, ReadOps, WriteOps, ReadQueue, WriteQueue are present .
Part of process performance metrics: It reflects how busy the disk subsystem is for each process. Internally, the storage engine updates these as it schedules I/Os. For instance, a high WriteQueue would indicate the storage server has many writes waiting on disk – a sign of disk bottleneck. These metrics feed into the Ratekeeper indirectly (Ratekeeper watches durable lag which is tied to disk speed) and also into operator monitoring (to alert on slow disks or overload).
RkUpdate
A Ratekeeper update event logging the cluster’s throttling status at a point in time. It contains the current Transactions Per Second limit (TPSLimit), the reason code for that limit, and other stats like worst storage queue or version lag. Essentially, RkUpdate encapsulates “how many transactions can we allow now and why?” for default-priority transactions.
Part of Ratekeeper (flow control): Ratekeeper runs on the cluster controller and periodically evaluates cluster health. RkUpdate is its decision broadcast – for example, if a storage server is nearly out of space or a log is slow, Ratekeeper will lower TPSLimit and log Reason=… (each reason code corresponds to e.g. storage space, log queue, CPU, etc.) . Internally, this directly affects the transaction start phase: proxies will allow at most TPSLimit transactions/sec. RkUpdate events allow operators to see when the database is throttling commits and what triggered it (e.g., reason 2 might mean worst storage queue too large).
RoleAdd
An event specifically denoting that a new role has been added to a process. This is essentially the “Transition=Add” case of the generic Role event, possibly broken out as its own type in newer versions. It will list which role was added (e.g., StorageServer on some UID).
Part of cluster controller’s role assignment: Whenever the cluster controller recruits a process to a role, that process logs RoleAdd. For instance, when a new storage server is recruited to remedy replication shortfall, it logs RoleAdd (SS). Internally this is tied to the data distribution and recovery processes – seeing RoleAdd for TLog, for example, means a new transaction log was brought online after a recovery. It provides a cleaner signal (compared to Role Refresh) for when the composition of the cluster changes.
MsgpackResizedBuffer
Indicates that a message-pack encoder/decoder buffer was resized. FDB uses MessagePack for efficient serialization in some components (possibly the Global Config or client info). This event logs that the buffer had to grow to accommodate data.
Part of client/server protocol internals: If FDB is sending a particularly large configuration or status object via message-pack, the buffer might auto-resize. This event is mostly diagnostic – it tells developers that their message size exceeded the initial or previous buffer length. While it doesn’t directly affect transactions, it could be relevant to configuration distribution (e.g., if tenant metadata or knob sync data is huge, seeing this could prompt increasing default buffer sizes to avoid reallocation overhead).
FileOpenError
An error event indicating failure to open a file. It will contain the file path and error code. For example, if the storage engine tries to open a data file and the file is missing or permissions are wrong, it logs FileOpenError.
Part of storage and backup operations: Internally, many components open files (storage engine data files, backup logs, etc.). A FileOpenError halts progress in that component – e.g., a storage server cannot start without its data file. This event is critical for troubleshooting – it likely surfaces during misconfiguration or disk failures. For the transaction subsystem, if a tLog can’t open its queue file, the cluster won’t be able to function (leading to recovery). Thus, this error is an immediate red flag.
TLogPeekAllCurrentOnly
Another TLog peek strategy event. “AllCurrentOnly” implies the storage server (or backup/DR) peeked from all active logs but only needed the current data (not historical). In practice, if data is fully in memory of all logs, the reader pulls from each log’s current portion and doesn’t need to consult any spilled (older) segments on disk.
Part of replication fetch optimization: Internally, if a storage server was behind and needed to catch up, it might have to consult multiple logs (if data is striped by tag across logs). This event confirms it read from all relevant logs and that those logs had the data readily available (no long-term history needed). It tells engineers that a full fan-in read occurred but stayed in the fast path (memory), thus likely minimal delay.
RkUpdateBatch
Similar to RkUpdate, but specifically for batch-priority transactions. Ratekeeper manages two pools: normal and batch priority. RkUpdateBatch logs the TPS limit for batch transactions and the reason for that limit . Often, batch limits come into play only after default transactions are throttled heavily.
Part of Ratekeeper’s throttling: Batch transactions (like backup reads or low-priority workloads) get their own limits so they don’t interfere with regular transaction throughput. Internally, RkUpdateBatch shows how many batch TPS are allowed and if a certain resource is the bottleneck (Reason code) . This influences the client’s behavior for any transactions tagged as batch priority – they will be delayed if the cluster is under strain. Operators can see via this event if batch workloads are being held back (e.g., to free up resources for normal txns).
DatabaseContextCreated
Logged on the client side (or in fdbserver if using the API there) when a new Database context is created. This happens when an application opens a connection to the cluster (calls fdb_open or similar). It often notes the cluster file used and an ID for the DB context.
Part of client operations: It marks the beginning of an application’s interaction with the database. Internally, creating a Database context spawns network threads (in the client) and starts the GRV cache/monitor etc. This event is useful for debugging client-side issues – for instance, if an app frequently opens/closing DBs, you’d see many of these. It has no effect on cluster state except each context will hold open connections to FDB coordinators/proxies.
TLogRestorePopped
This event likely appears in DR (Disaster Recovery) or backup scenarios. It indicates that a remote TLog (log router or backup feed) acknowledged pop of log data after a restore. In other words, the system tracking how far a backup or DR target has caught up can “restore” or set the popped version for those logs once they’re done.
Part of backup/DR replication: Internally, FDB’s DR agents copy mutations from primary TLogs to secondary cluster. Once applied, the secondary’s log routers can pop data. TLogRestorePopped on the primary may denote it’s safe to drop those mutations intended for the secondary because the secondary signaled completion. It’s essentially freeing resources on the primary after a DR cycle. In the context of normal operations, it’s rarely seen unless a DR was configured.
StorageServerSourceTLogID
Indicates which TLog (by ID) a storage server is currently consuming from as its source of mutations. A storage server normally subscribes to a set of TLogs (for redundancy), but during certain events (like recovery), it might temporarily switch or mark one as primary source. This event logs such an association or change.
Part of replication topology: It helps track the mapping of storage servers to TLog IDs. Internally, during recovery the cluster controller reassigns storage servers to new TLogs. When a storage server starts pulling from a particular TLog, it logs this to ensure the chain of custody for mutations is known. It’s important in debugging replication problems – e.g., if a storage server is not getting updates, knowing which TLog it was supposed to pull from (via this event) helps locate the issue.
TLogRejoining
Logged when a previously removed or disconnected TLog is rejoining the cluster’s log system. For instance, if a log process temporarily fell out (e.g., in a recovery), but wasn’t completely lost and is now coming back into service, it might log that it’s rejoining. This could include the log’s ID and role.
Part of fault tolerance and recovery: Internally, if a transient failure occurs, FDB might allow a TLog to rejoin to avoid losing data (this is more applicable to the new generation of coordinators or if using a recovery that doesn’t fully discard old logs). It signals that the log data on that process is being reintegrated. This can occur in multi-region replication as well if a remote log comes back online. The event is important to ensure that no data duplication or gap occurs – it means the system is carefully handing off between old/new log processes.
TeamCollectionInfo
A trace event from the Data Distributor that provides information about the state of storage server team collections. It may log the number of teams, number of unhealthy teams, etc., similar to the ServerTeamInfologged via triggerddteaminfolog in fdbcli .
Part of data distribution: This is a snapshot of the team health. Internally, the Data Distributor continuously ensures there are enough teams and that data is balanced. TeamCollectionInfo might be logged when significant team changes happen or periodically. It relates to replication: for instance, it could show “teams: 100 total, 0 unhealthy, teamSize: 3” indicating triple replication teams are all healthy. If teams are unhealthy (missing a member), DD knows to recruit new storage servers – so this event is an insight into that state machine.
PeekMetrics
Metrics about TLog peeking operations. This could include counts of how many bytes were read via peek, how long peeks waited, etc., over some interval. Essentially, it quantifies the effort storage servers (and backup/DR) spend pulling data from logs.
Part of transaction data propagation: Internally, if peeking becomes slow (e.g., storage servers falling behind), these metrics will reflect large read sizes or long waits. PeekMetrics help Ratekeeper or the cluster controller indirectly by highlighting if log readers are a bottleneck. For example, if a storage server has to repeatedly read a large amount from a log (maybe due to being behind), peek metrics would spike – indicating stress in the storage server’s fetch from TLogs.
SimpleFileRename
Logged when a file is renamed via FDB’s FileSystem abstraction. FDB might rename files during log rotation or snapshotting (e.g., creating a backup file then moving it into place). This event captures the old name and new name.
Part of file maintenance: Internally used by backup and maybe the storage engine (Redwood can use an atomic rename of new pages file). It ensures that a critical file operation completed. In backup systems, a SimpleFileRename event might occur when finalizing a backup file (rename temp to final). If this fails, those systems know via this event (or lack thereof). It’s also used on log file rotation (foundationdb traces or external log files).
TLogJoinedMe
Likely logged on a cluster controller or master to indicate a new TLog joined the cluster and registered with it. The phrasing “JoinedMe” suggests the perspective of the master: a TLog says “I’ve joined.” It might include the TLog’s ID that joined.
Part of recovery: During recovery, the master recruits TLogs. When each TLog successfully joins (connects and starts accepting versions), the master (or CC) logs TLogJoinedMe. Internally this means the TLog is now part of the replication topology for new transactions. This event essentially closes the loop for a TLog recruitment – from the controller’s view, “I now have this TLog in my team.” It assures the transaction system that the required logs are active.
WorkerRegister
Emitted by the cluster controller when a new FDB process (worker) registers. Every fdbserver on startup contacts the cluster controller and says “I’m alive” along with its locality info. This event logs that occurrence, often including the worker’s network address and zones.
Part of cluster coordination: It’s the foundation of role assignment. Internally, once a worker registers, the controller may assign it roles. Thus WorkerRegister is the first step in scaling out – it signifies a process is ready to do work. If a process dies, it will stop heartbeating and eventually be considered gone. So, WorkerRegister (and associated events like WorkerRemoved) let operators trace changes in the cluster’s process membership. It is critical for availability – e.g., on rolling upgrades, each new process should log this upon startup to confirm it joined the cluster.
CoordinationPing
A periodic ping from FDB processes to the coordination state (i.e., the cluster coordinators). This event often includes a timestamp or latency for contacting a coordinator or writing to the coordinated state in etcd/SQLite. It’s emitted by the cluster controller or other roles to ensure the coordinators are responsive .
Part of cluster consistency (coordinators): FDB uses a Paxos-like coordination. CoordinationPing events help detect “gray” coordinators – if pings start timing out, the cluster might initiate a recovery with new coordinators. Internally, each coordinator client (like the cluster controller) sends a heartbeat (read/write) to the coordination data. This event logs those and any delay. In effect, it monitors the health of the foundation of the transaction lifecycle – if coordinators are slow, getting a read version or committing might stall.
MonitorTenantsOverStorageQuota
Logged by the cluster’s background task that monitors tenant disk usage against their quotas. It likely runs periodically and, if any tenant has exceeded its allocated storage quota, this event would note that (possibly listing tenant IDs that are over quota).
Part of multi-tenant operations: Internally, FDB tracks the byte usage per tenant. This monitor ensures no tenant wildly exceeds limits (which could threaten cluster storage capacity). When a tenant is over quota, FDB can trigger warnings or limit further data. The event provides visibility, so operators know which tenants to investigate. It ties into client operationsbecause exceeding quota could cause writes from that tenant to be rejected until space is freed.
RoleRemove
The counterpart to RoleAdd – this event logs that a role was removed from a process. For example, if a storage server role is ended (perhaps the process is shutting down or a rebalancing made it redundant), it logs RoleRemove for StorageServer. It includes the role name and maybe the reason (if available).
Part of role lifecycle: Internally, when the cluster controller decides a process should stop a role (maybe during failover or team reconfiguration), it commands that and logs RoleRemove. This ensures that the system (and operators) know that process is no longer serving that role. It’s important for correctness – e.g., a storage server removed event means its data movement (if any) should be complete. For the transaction subsystem, removing a proxy or log role will precede a recovery. For data distribution, removing a storage role means data has been safely relocated.
ProxyMetrics
A periodic report from a commit proxy with metrics about transaction processing. It includes counts like TxnStartIn/Out (transactions started), commit batch sizes, mutations handled, conflicts detected, etc . These metrics give insight into how many transactions the proxy is managing and how efficient conflict resolution is.
Part of the commit pipeline: Proxies are stateless processes that batch and forward transactions to resolvers and logs. ProxyMetrics are essential for performance monitoring – they show, for instance, if proxies are saturated (huge numbers of transactions started vs committed) . Internally, these metrics feed the cluster’s status reports and help Ratekeeper or Data Distributor indirectly (e.g., if commits are very high, maybe throttling or splitting load across more proxies is needed). They also break down the types of transactions (default, batch, system) to ensure each priority is handled . In short, ProxyMetrics reflect the transaction commit throughput and latency from the proxy’s perspective.
DDRecruiting
Indicates the Data Distributor is actively recruiting storage servers for new teams . The event shows the state of recruitment (e.g., “Sending request to CC”) and how many exclusions are in effect . It’s logged when DD notices it needs more storage servers (due to failure or wanting to rebalance).
Part of data distribution / healing: Internally, when replication level is low or new servers are needed for balance, DD asks the cluster controller to recruit. DDRecruiting traces that moment, including if any zones are excluded from recruitment (perhaps to avoid recruiting on the same machine as an existing replica) . This event is crucial in scenarios of machine failures: you’ll see DDRecruiting as FDB works to add storage on remaining machines to restore redundancy. It relates to the storage engine indirectly by ensuring capacity and fault tolerance.
WorkPool_Stop
Logged when a WorkPool (a pool of worker threads) is stopped. FDB uses thread pools for certain tasks outside of the main event loop (for example, the backup agent’s file I/O or encryption tasks). This event indicates that such a thread pool is being shut down, likely as the process or service stops.
Part of internal threading for aux operations: While core FDB is single-threaded for main roles, some features (backup, TLS handshakes, etc.) use thread pools. WorkPool_Stop event confirms all tasks have drained and threads exited. Internally, this ensures a clean shutdown (no dangling threads). From a client/server operations perspective, this would be seen when, say, the backup process stops or a special API (like fast compression threads) ends – confirming those resources are freed.
GetMappedRangeLocalMetrics
Metrics specifically for the local portion of GetMappedRange calls. GetMappedRange is an API that does a primary range read and then does secondary gets for each result . “LocalMetrics” likely measure the performance of cases where the secondary reads were served by local storage (same region).
Part of transaction read optimization: These metrics allow evaluation of how efficient the mapped range is when data is local. Internally, FDB can perform secondary reads either locally or, if data resides in another region, remotely. High local success means low latency. GetMappedRangeLocalMetrics might include count of keys processed locally and latency distribution for those local get operations. It ties into the client API throughput – if most secondary reads are local, the new API is achieving its goal of bundling reads efficiently.
ReadQueueWaitMetrics
Metrics about how long reads spend waiting in the storage server’s queue. Each storage server has a limited number of read threads (in the single-threaded model it interleaves tasks). If many read requests come in concurrently, later ones wait. This event aggregates those wait times.
Part of storage engine workload: If ReadQueue wait time grows, it indicates the storage server is saturated serving reads. Internally, this can happen due to slow disk or simply too many reads. These metrics inform Ratekeeper or data distribution indirectly – for instance, persistent high read queue waits on a storage server might mark it as “busy” for read load and trigger read-hotspot mitigation. For clients, high wait means higher read latency. So this is critical for the transaction read path performance.
UpdateLatencyMetrics
Metrics on the latency of update (write) operations on storage servers. Likely this measures how long it takes from when a mutation batch is received from the TLog until it’s applied to the storage engine and acknowledged durable. It could provide a distribution of these latencies.
Part of transaction commit path: Once a commit’s mutations reach storage servers, applying them quickly is key for freeing TLog space and being ready for reads. UpdateLatencyMetrics exposes if storage servers are slow to apply writes (e.g., due to slow disk I/O when writing to KV store). Internally, if this latency spikes, Ratekeeper may throttle commits (because the durable lag on that storage server will grow). Thus it’s a direct window into the storage engine’s write performance, affecting overall commit throughput.
GetKeyMetrics
Metrics about getKey operations (which find the boundary key before or after a given key). This likely tracks how many getKey calls were processed and their latency. getKey is used in scans and by directories to find partition boundaries, so it can be frequent.
Part of read path: If getKey calls start taking long (maybe because they have to scan a large span for the next key), these metrics will reflect that. Internally, it might help identify big gaps in key space or inefficient use of getKey (like linear scans). It informs the transaction subsystemperformance – e.g., the directory layer in RecordLayer heavily uses getKey; if those are slow, overall throughput suffers. These metrics help pin down such issues to the operation type.
ReadLatencyMetrics
General metrics on read latency as observed by the storage server. This likely aggregates all types of reads (point gets, range reads) into a latency distribution. It might segment by size or type, but overall gives the storage-side view of how quickly it serves reads.
Part of storage server performance: This is crucial for user experience – high read latencies degrade transaction times. Internally, the storage server might log p50, p90, max latencies for reads. This can highlight if a certain storage node’s disk is slower than others or if large range reads are causing spikes. It feeds into the client observable metrics (FDB clients measure their own, but this is the server’s perspective). It can also influence data distribution: if one node consistently has higher read latencies (maybe due to load), it might be a target for moving some data away.
GetMappedRangeMetrics
Metrics for the overall GetMappedRange calls (the combined operation). It would measure the end-to-end cost of performing a mapped range read, including both the primary range scan and all secondary lookups. This likely includes the total keys scanned and total values fetched and the time taken.
Part of advanced client operations: GetMappedRange is meant to improve throughput by batch-processing what used to be many separate gets . These metrics verify that – they show how long a mapped range takes and perhaps how it scales with number of results. Internally, this helps the FDB team assess if the feature is working (e.g., 4x throughput improvement as expected ). For the transaction system, if these metrics show high latency, it might push to optimize either the mapper or the parallelism of secondary gets.
GetMappedRangeRemoteMetrics
Metrics for the portion of GetMappedRange where secondary reads had to go to remote storage servers (e.g., another region). This will isolate the added cost when data isn’t local. It might track how many remote requests were issued and additional latency incurred.
Part of multi-region read optimization: If a mapped range query frequently hits remote data, these metrics will capture the penalty. Internally, FDB can use this to decide if it’s worth it to perform some form of caching or data movement for such patterns. It highlights how often the “fast path” (local) versus “slow path” (remote) is taken in the transaction read lifecycle for complex reads. For users, if these metrics are high, it implies their workload isn’t perfectly localized – possibly an argument to adjust key placement or tenant locality.
KVGetRangeMetrics
Low-level metrics from the storage engine’s key-value store for range scans. It likely measures the scanning performance at the storage engine (KV) layer, distinct from higher-level GetRange which might involve sending data over network. This could include how many pages were read from disk, or time taken inside the storage engine to fulfill range reads.
Part of storage engine internals: By separating KVGetRange (engine) from overall GetRange (which includes network to client), FDB can pinpoint bottlenecks in the engine (e.g., RocksDB or Redwood). Internally, if KVGetRange is slow, it might indicate a lot of disk seeks or a large range traversed. This metric helps developers tune the engine (like Redwood’s page read-ahead). In the bigger picture, it affects transaction reads – slow engine range reads = slow user getRange.
GetValueMetrics
Metrics about point Get operations (getValue) from the storage server’s perspective. It counts and times how those are handled. A point get is usually served from memory (if key is in cache) or requires one disk seek if not. This metric will show the distribution of get times.
Part of transaction read path: Most read operations in transactions are point gets. Internally, storage servers optimize heavily for these. GetValueMetrics showing high latency could mean cache misses forcing disk reads or an overwhelmed storage server. It informs whether the storage engine’s caching is sufficient (most gets should be very fast). It can also help identify if some keys are hot – although that’s usually in different events, an extremely high count on specific storage servers’ GetValue vs others could hint a hotspot.
GetRangeMetrics
High-level metrics for range reads (getRange) as seen by the storage server (including network sending of results). It would measure how many ranges, how many rows returned, and latency. Since range reads can be arbitrarily large, metrics likely include average bytes or rows per range request.
Part of transaction reads: Range queries often retrieve many KV pairs, potentially straining both server (CPU/IO) and client (network). Internally, GetRangeMetrics helps FDB decide if it needs to e.g. paginate more aggressively or if a client is requesting very large ranges. It ties into load balancing – extremely heavy range reads on one storage node could slow others, so DD might split shards or move some. Thus these metrics close the loop between storage engine performance and data distribution decisions for read-heavy workloads.
ReadVersionWaitMetrics
Metrics capturing how long transactions had to wait to get a read version (Read Version, a.k.a. GRV). Normally, getting a read version from proxies is fast (<1 ms). However, during recovery or if proxies are backed up, clients might wait. This metric accumulates those wait times/distributions.
Part of transaction start: If the cluster is healthy, this will be near zero. Internally, this is critical during issues: for example, if a recovery is happening, all transactions queue for a new read version – the wait could be seconds, and these metrics record that. It also helps identify if proxies are overloaded (if they start queuing GRV requests). In terms of client-observable behavior, ReadVersionWait is the first latency a transaction encounters; prolonged waits here directly translate to slower transaction times or even timeouts. Monitoring this helps ensure the coordination layer (master/proxies) is keeping up with load.
RocksDBMetrics
A collection of metrics specific to the RocksDB storage engine integration. This likely includes stats like number of sst files, memtable sizes, compaction events, cache usage, bloom filter utility, etc. It may also surface in the Status JSON . Examples might be “RocksDBNumImmutableMemtables” or “RocksDBBackgroundCompactions” and their counts.
Part of storage engine (Sharded RocksDB) monitoring: Internally, these metrics are vital to understanding how the engine is behaving under FDB’s workload. For instance, if compaction metrics are high, it might explain write latency spikes. If RocksDB block cache hit-rate is low, it explains read performance issues. These metrics allow FDB operators to tune engine knobs via the new knob system (many RocksDB options can be adjusted). They were added to status for visibility . In sum, RocksDBMetrics events tie the underlying LSM engine’s health to FDB’s higher-level performance, ensuring that issues like compaction stall or excessive write amplification can be detected and addressed.



